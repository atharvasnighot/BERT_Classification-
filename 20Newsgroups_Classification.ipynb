{"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"","display_name":""}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Newsgroup Document Classification with DistilBERT","metadata":{}},{"cell_type":"markdown","source":"Newsgroup documents play an important role in various aspect of information management and decision-making processes.\n\nIn this mini-project, I will explore how transformers (DistilBERT) can be used to predict newsgroups (the classes) from the text content of documents.\n\nTransformer models can also be used in other predictive modeling applications related to information retrieval and content recommendation.\n\nIn addition, I will explore the impact of stopwords & metadata removal on the performance of the classifier.\n\n**Summary of Contents:**\n\n1. Setup & Data Preparation\n2. Data Pre-processing: Tokenization and Data Loader Construction\n3. Initialise Pre-trained DistilBERT Model & Transfer Learning\n4. Model Training & Evaluation\n5. Findings & Conclusion\n\n\nNote: This project was originally run on Google Colab.","metadata":{}},{"cell_type":"markdown","source":"# 1. Setup & Data Preparation","metadata":{}},{"cell_type":"code","source":"# installing pytorch lightning and transformer\n! pip install --quiet lightning\n! pip install --quiet transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Update the path to where the data file is uploaded on Google Drive.","metadata":{}},{"cell_type":"code","source":"# For Google Colaboratory\nimport sys, os\nif 'google.colab' in sys.modules:\n    # mount google drive\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    path_to_file = '/content/gdrive/My Drive/DistilBertClassification'\n\n    print(path_to_file)\n    # move to Google Drive directory\n    os.chdir(path_to_file)\n    !pwd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# importing packages\nfrom os import listdir\nfrom os.path import join\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torchmetrics\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom transformers import AutoModelForSequenceClassification\nfrom pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preparation\n\nIn this mini-project, a subset of the newsgroup dataset is utilised.\n\nNewgroups utilised: 'rec.sport.baseball', 'comp.graphics', 'sci.space', 'talk.religion.misc'","metadata":{}},{"cell_type":"code","source":"# each text document belongs to a group which denotes the class label\n# all documents belonging to the same class are in one folder\nfolders = ['rec.sport.baseball', 'comp.graphics', 'sci.space', 'talk.religion.misc'] #\n\n# Define a helper function to gather the pathnames of documents in the dataset, and the label is the name of the folder\ndef get_data(folders):\n    my_path = '20_newsgroups'\n    files = []\n    # Extract names of files in each folder\n    for folder_name in folders:\n        folder_path = join(my_path, folder_name)\n        files.append([f for f in listdir(folder_path) if not f.startswith('.')])\n\n    pathname_list, Y = [], []\n    for fo in range(len(folders)):     # each folder\n        for fi in files[fo]:           # each file in folder\n            pathname_list.append(join(my_path, join(folders[fo], fi)))\n            Y.append(folders[fo])\n\n    return pathname_list, Y\n\n# Get list of pathnames & labels\npathname_list, Y = get_data(folders)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Divide the dataset into training, validation, and testing sets using a split ratio of 60-20-20.","metadata":{}},{"cell_type":"code","source":"# Divide the dataset into training, validation, and testing sets using a split ratio of 60-20-20.\nX_train_path, X_temp_path, y_train, y_temp = train_test_split(pathname_list, Y, train_size=0.6, random_state=88)\nX_val_path, X_test_path, y_val, y_test = train_test_split(X_temp_path, y_temp, test_size=0.5, random_state=88)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Pre-processing: Tokenization and Data Loader Construction","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Removing Stopwords & Pre-processing the Text Data (X)","metadata":{}},{"cell_type":"markdown","source":"The document contains many meaningless words and symbols. For each document, a function `preprocess_sub` defined below will be applied to remove the metadata and filter out punctuation, quotations, tabs, short words, and stop words.","metadata":{}},{"cell_type":"code","source":"# Define list of stopwords\nstopwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by',\n 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n 'each', 'few', 'for', 'from', 'further',\n 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such',\n 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\",\n \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very',\n 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\",\n 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves',\n 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n\n# Helper function to remove stopwords\ndef preprocess_sub(words):\n    #filter out some unnecessary data like tabs\n    table = str.maketrans('', '', '\\t')\n    words = [word.translate(table) for word in words]\n\n    # the character: ' is removed from the list of symbols that are to be discarded from the documents\n    punctuations = (string.punctuation).replace(\"'\", \"\")\n    trans_table = str.maketrans('', '', punctuations)\n    stripped_words = [word.translate(trans_table) for word in words]\n\n    # remove white spaces\n    words = [str for str in stripped_words if str]\n\n    # unquote quoted words\n    p_words = []\n    for word in words:\n        if (word[0] and word[len(word) - 1] == \"'\"):\n            word = word[1:len(word) - 1]\n        elif (word[0] == \"'\"):\n            word = word[1:len(word)]\n        else:\n            word = word\n        p_words.append(word)\n\n    words = p_words.copy()\n\n    # remove white spaces\n    words = [str for str in words if str]\n\n    # remove just-numeric strings as they do not have any significant meaning in text classification\n    words = [word for word in words if not word.isdigit()]\n\n    #  remove words with less than 2 characters\n    words = [word for word in words if len(word) > 2]\n\n    # normalize the cases of our words\n    words = [word.lower() for word in words]\n\n    # remove stop words, which do not have any significant meaning in text classification\n    words = [word for word in words if not word in stopwords]\n\n    return words\n\n# Helper functions to remove metadata\ndef remove_metadata(lines):\n    for i in range(len(lines)):\n        if(lines[i] == '\\n'):\n            start = i+1\n            break\n    new_lines = lines[start:]\n    return new_lines\n\n# Consolidated function to preprocess a document, including removing stopwords and metadata\ndef preprocess(doc):\n    list_of_words = []\n    for path in doc:\n        #print(path)\n        f = open(path, 'r', encoding='utf-8')\n        text_lines = f.readlines()\n\n        # remove the meta-data at the top of each document\n        text_lines = remove_metadata(text_lines)\n\n        doc_words = []\n        for line in text_lines:\n            words = line[0:len(line) - 1].strip().split(\" \")\n            words = preprocess_sub(words)\n            doc_words.extend(words)\n        list_of_words.append(' '.join(doc_words))\n\n    return list_of_words\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert from file pathname to one-line strings and remove stopwords / metadata using the \"preprocess\" function\nX_train_cleaned = preprocess(X_train_path)\nX_val_cleaned = preprocess(X_val_path)\nX_test_cleaned = preprocess(X_test_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Encode the Labels (Y)\n\nSubsequently, the document labels (y) are encoded to numerical values.","metadata":{}},{"cell_type":"code","source":"# Encode Labels (Y) to numerical values\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder on y_train\nencoded_labels = label_encoder.fit(y_train)\n\n# Transform labels to numerical values\ny_train_encoded = encoded_labels.transform(y_train)\ny_val_encoded = encoded_labels.transform(y_val)\ny_test_encoded = encoded_labels.transform(y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The labels are encoded as such:\n* 0: comp.graphics\n* 1: rec.sport.baseball\n* 2: sci.space\n* 3: talk.religion.misc\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Tokenise using Pre-trained DistilBERT & Construct DataLoader\n\nUse pre-trained \"distilbert-base-uncased\" tokenizer to tokenize the dataset.\n\nNote: DistilBERT (`distilbert-base-uncased`) has an input length of 512 tokens. Hence, the tokenised sequences are padded to a length of 512.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nclass TextDataset(Dataset):\n    def __init__(self, X, y, tokenizer):\n        self.X = X\n        self.y = y\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        # Tokenize text\n        text = self.X[idx]\n\n        tokenized_text = self.tokenizer(text, truncation=True, padding='max_length', max_length=512)   # pad to length of 512, in line with the input length of \"distilbert-base-uncased\"\n        # Tokenizer input max length: 512   # Tokenizer vocabulary size: 30522\n        # truncation=True: This parameter ensures that if the length of the tokenized input exceeds the maximum sequence length supported by the model (in this case, 512 tokens), it will truncate the input sequence.\n        # padding=True: This parameter ensures that the tokenized sequences are padded to the same length using padding tokens. This is necessary because neural networks typically expect inputs of the same length\n\n        # Convert to torch tensors\n        input_ids = torch.tensor(tokenized_text['input_ids'])\n        attention_mask = torch.tensor(tokenized_text['attention_mask'])\n        label = torch.tensor(self.y[idx])\n\n        # Create dictionary\n        data_dict = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"label\": label\n        }\n\n        return data_dict\n\n# Creating tokenizer\n# Use pre-trained \"distilbert-base-uncased\" tokenizer to tokenize the dataset\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\n\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n\nYou will be able to reuse this secret in all of your notebooks.\n\nPlease note that authentication is recommended but still optional to access public models or datasets.\n\n  warnings.warn(\n"}]},{"cell_type":"markdown","source":"Create corresponding dataloaders for each set to facilitate training and evaluation processes","metadata":{}},{"cell_type":"code","source":"# Creating datasets\ntrain_dataset_cleaned = TextDataset(X_train_cleaned, y_train_encoded, tokenizer)\nval_dataset_cleaned = TextDataset(X_val_cleaned, y_val_encoded, tokenizer)\ntest_dataset_cleaned = TextDataset(X_test_cleaned, y_test_encoded, tokenizer)\n\n# Create corresponding dataloaders for each set to facilitate training and evaluation processes.\n# Set a batch size of 10\n\n# Create DataLoaders for each set\ntrain_loader_cleaned = DataLoader(\n    dataset=train_dataset_cleaned,\n    batch_size=10,\n    shuffle=True,\n    num_workers=2\n)\n\nval_loader_cleaned = DataLoader(\n    dataset=val_dataset_cleaned,\n    batch_size=10,\n    num_workers=2\n)\n\ntest_loader_cleaned = DataLoader(\n    dataset=test_dataset_cleaned,\n    batch_size=10,\n    num_workers=2\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Initialise Pre-trained DistilBERT Model & Transfer Learning\n","metadata":{}},{"cell_type":"markdown","source":"Initialise the pretrained model DistilBert model and set the number of labels.","metadata":{}},{"cell_type":"code","source":"# Get the pretrained model DistilBert model from AutoModelForSequenceClassification and set the number of labels\npretrained_distilbert = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=4)   # 4 possible labels\n\npretrained_distilbert\n\n# Note based the description that the weights of ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight'] are not loaded --> to train these 2 layers","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0-5): 6 x TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=4, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{}}]},{"cell_type":"markdown","source":"For transfer learning using DistilBERT, only the Linear classifier layers are unfrozen. The pretrained Transformer layers are kept frozen & applied for feature extraction.\n\nNote: as the DistilBERT model is quite large, finetuning of the entire model will require a large amount of computational resources.","metadata":{}},{"cell_type":"code","source":"# Freeze all layers\nfor param in pretrained_distilbert.parameters():\n    param.requires_grad = False\n\n# Unfreeze only the pre_classifier & classifier layers\nfor param in pretrained_distilbert.pre_classifier.parameters():\n    param.requires_grad = True\n\nfor param in pretrained_distilbert.classifier.parameters():\n    param.requires_grad = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, define the PyTorch Lightning model, including:\n\n- Implementing the training, validation and testing procedure\n- Define loss function for the multi-class classification problem (CrossEntropyLoss), and use Accuracy for evaluation\n- Loss function & Accuracy are logged to allow monitoring of the training process using Tensorboard\n\nA low learning rate is used to prevent overfitting of the pre-trained DistilBERT model.","metadata":{}},{"cell_type":"code","source":"# Define Classification Lightning Model\n\nclass TextClassificationModel(pl.LightningModule):\n    def __init__(self, model, learning_rate=5e-5):   # set a low learning rate to prevent overfitting of the pre-trained model / disrupting its pretrained-weights\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        self.model = model ## to use pretrained DistilBERT model\n\n        # Classification Metrics\n        self.acc_score = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)    # 4 label classes\n\n        # Define loss function\n        self.ce_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels):\n        # Forward pass through the pre-trained model\n        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n\n    def training_step(self, batch, batch_idx):\n        # Gather outputs\n        outputs = self.forward(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"])\n        logits = outputs[\"logits\"]  #outputs.logits\n        labels = batch[\"label\"]  # actual label\n\n        # Calculate cross-entropy loss\n        loss = self.ce_loss(logits, labels)\n        self.log(\"train_loss\", loss, prog_bar=True)\n\n        # Predict class\n        predicted_labels = torch.argmax(logits, 1)\n        # Calculate Accuracy\n        accuracy = self.acc_score(predicted_labels, labels)\n        self.log(\"train_acc\", accuracy, prog_bar=True)\n\n        return loss #outputs[\"loss\"]  # this is passed to the optimizer for training\n\n    def validation_step(self, batch, batch_idx):\n        # Gather outputs\n        outputs = self.forward(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"])\n        logits = outputs[\"logits\"]  #outputs.logits\n        labels = batch[\"label\"]  # actual label\n\n        # Calculate cross-entropy loss\n        loss = self.ce_loss(logits, labels)\n        self.log(\"val_loss\", loss, prog_bar=True)\n\n        # Predict class\n        predicted_labels = torch.argmax(logits, 1)\n        # Calculate Accuracy\n        accuracy = self.acc_score(predicted_labels, labels)\n        self.log(\"val_acc\", accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        # Gather outputs\n        outputs = self.forward(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"])\n        logits = outputs[\"logits\"]  #outputs.logits\n        labels = batch[\"label\"]  # actual label\n\n        # Predict class\n        predicted_labels = torch.argmax(logits, 1)\n        # Calculate Accuracy\n        accuracy = self.acc_score(predicted_labels, labels)\n        self.log(\"test_acc\", accuracy, prog_bar=True)\n\n        print(labels)\n        print(predicted_labels)\n        print(accuracy)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model Training & Evaluation","metadata":{}},{"cell_type":"markdown","source":"Initialise the classification model, define best model checkpoint & Trainer. The model will be trained over 10 epochs.","metadata":{}},{"cell_type":"code","source":"# Create a similar model using the predefined \"TextClassificationModel\" from Step 3\ntext_classification_model_cleaned = TextClassificationModel(pretrained_distilbert)\n\n#specify logger\nlogger = TensorBoardLogger(\"distilbert/\", name=\"finetuning_cleaned\", version=\"v1\")\n\n# define model checkpoint callback\ncallbacks_cleaned = [\n    ModelCheckpoint(\n        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n    )  # save top 1 model\n]\n\n#define trainer -- add callback\ntrainer_cleaned = pl.Trainer(\n    max_epochs=10,\n    callbacks=callbacks_cleaned,\n    accelerator=\"gpu\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=1,\n)\n\n# train the model\ntrainer_cleaned.fit(model=text_classification_model_cleaned,\n            train_dataloaders=train_loader_cleaned,\n            val_dataloaders=val_loader_cleaned)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\nINFO:pytorch_lightning.callbacks.model_summary:\n\n  | Name      | Type                                | Params\n\n------------------------------------------------------------------\n\n0 | model     | DistilBertForSequenceClassification | 67.0 M\n\n1 | acc_score | MulticlassAccuracy                  | 0     \n\n2 | ce_loss   | CrossEntropyLoss                    | 0     \n\n------------------------------------------------------------------\n\n593 K     Trainable params\n\n66.4 M    Non-trainable params\n\n67.0 M    Total params\n\n267.826   Total estimated model params size (MB)\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae59ce0ef0994f55a85af18fa4aeccd9","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"762efad2e2ca4191afec0fbd21dc3db3","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"854a39c4f78a453f95a49f9942e1f0ac","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a570360a97f4502bd3ccc8d7ecf4cd2","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"950dc05360ef47e58515e1641abe8ab0","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49c7d7912ba947c4b90765a5baa40402","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"018d67d4180847cca10c0f8ecc7b2b9c","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bccb4ecf28f4eee8ef7d95a19a72280","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c7455819c03a443188452818425f8c05","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cf87491625d4e5db9e1a1d69e2355a0","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd584e2e13324f5d8bd12dbaac2927b7","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6a2ac90928d4ff1a25dd1f35e9791e1","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n"}]},{"cell_type":"markdown","source":"With the newly preprocessed data to remove metadata & stopwords, there was an improvement in the test accuracy from 91.25% to 92.5%.","metadata":{}},{"cell_type":"code","source":"# Run the best model using the test data\n\nbest_model_path_cleaned = callbacks_cleaned[0].best_model_path\nprint(best_model_path_cleaned)\n\ntrainer_cleaned.test(ckpt_path=best_model_path_cleaned, dataloaders=test_loader_cleaned)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at distilbert/finetuning_cleaned/v1/checkpoints/epoch=8-step=216.ckpt\n"},{"name":"stdout","output_type":"stream","text":"distilbert/finetuning_cleaned/v1/checkpoints/epoch=8-step=216.ckpt\n"},{"name":"stderr","output_type":"stream","text":"INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\nINFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from the checkpoint at distilbert/finetuning_cleaned/v1/checkpoints/epoch=8-step=216.ckpt\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce1a196047b64efcb6846c194bf9751d","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stdout","output_type":"stream","text":"tensor([2, 0, 3, 0, 2, 2, 0, 2, 1, 1], device='cuda:0')\n\ntensor([2, 0, 3, 0, 3, 2, 0, 2, 3, 1], device='cuda:0')\n\ntensor(0.8000, device='cuda:0')\n\ntensor([2, 2, 1, 0, 2, 0, 1, 0, 0, 3], device='cuda:0')\n\ntensor([2, 2, 1, 0, 2, 0, 1, 0, 0, 3], device='cuda:0')\n\ntensor(1., device='cuda:0')\n\ntensor([3, 2, 1, 1, 3, 2, 1, 0, 3, 1], device='cuda:0')\n\ntensor([3, 2, 1, 1, 3, 2, 1, 0, 3, 1], device='cuda:0')\n\ntensor(1., device='cuda:0')\n\ntensor([1, 3, 2, 1, 3, 1, 3, 0, 1, 2], device='cuda:0')\n\ntensor([3, 3, 2, 1, 3, 1, 3, 0, 1, 2], device='cuda:0')\n\ntensor(0.9000, device='cuda:0')\n\ntensor([3, 1, 1, 2, 3, 1, 0, 2, 1, 1], device='cuda:0')\n\ntensor([3, 1, 1, 2, 3, 1, 0, 2, 1, 1], device='cuda:0')\n\ntensor(1., device='cuda:0')\n\ntensor([2, 2, 2, 3, 3, 2, 2, 0, 3, 1], device='cuda:0')\n\ntensor([2, 2, 3, 3, 3, 2, 2, 0, 3, 1], device='cuda:0')\n\ntensor(0.9000, device='cuda:0')\n\ntensor([3, 0, 1, 0, 0, 0, 0, 2, 2, 3], device='cuda:0')\n\ntensor([3, 0, 1, 2, 0, 0, 0, 2, 2, 3], device='cuda:0')\n\ntensor(0.9000, device='cuda:0')\n\ntensor([3, 3, 2, 1, 2, 1, 1, 3, 0, 2], device='cuda:0')\n\ntensor([3, 3, 2, 3, 2, 1, 1, 3, 0, 2], device='cuda:0')\n\ntensor(0.9000, device='cuda:0')\n"},{"output_type":"display_data","data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.925000011920929     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.925000011920929    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":["[{'test_acc': 0.925000011920929}]"]},"metadata":{}}]},{"cell_type":"markdown","source":"# 5. Findings & Conclusion","metadata":{}},{"cell_type":"markdown","source":"After training both models with 10 epochs, the model trained on the data with additional preprocessing to remove stopwords performed slightly better, with a test accuracy of 92.50%, as compared to 91.25% for the initial model without additional preprocessing.\n\n*(Note: there was a previous model that was trained without performing stopword removal. The code for this was not uploaded)*\n\nHowever, while observing the tensorboard trend of training and validation accuracy/loss for both models, the initial model appeared to be performing better with higher validation accuracy after 10 epochs.\n\nThis contrast in observation was observed likely because stopwords removal, which include commonly used words/tokens without much semantic meaning, likely helped the classification model focus on more meaningful words or tokens. Hence, allowing the model trained after stopwords removal to learn more robust and meaningful representations of text, leading to better generalisation and classification accuracy on the testing data.\n\nIn contrast, the initial model without additional text preprocessing may have overfitted on the noise or common stopwords in the training data, thus it did not perform or generalise well on the test data.\n","metadata":{}}]}